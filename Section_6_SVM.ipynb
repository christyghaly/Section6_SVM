{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a very powerful Machine Learning model, capable of performing linear or nonlinear classification and even outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hard Margin Classification\n",
    "The following figure describes the idea of support vector machines in which, Two classes from iris datasets were used with two features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"HardMargin.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The\n",
    "model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on\n",
    "this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier;\n",
    "this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes.\n",
    "This is called large margin classification.Notice that adding more training instances “off the street” will not affect the decision boundary at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Sensitivity\n",
    "SVMs are sensitive to the feature scales, as you can see in the following\n",
    "Figure: on the left plot, the vertical scale is much larger than the\n",
    "horizontal scale, so the widest possible street is close to horizontal.\n",
    "After feature scaling (e.g., using Scikit-Learn’s StandardScaler),\n",
    "the decision boundary looks much better (on the right plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='scale_sensitivity.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.5 52.5]\n",
      "[[ 0.90453403 -1.5011107 ]\n",
      " [-1.50755672 -0.11547005]\n",
      " [-0.30151134  1.27017059]\n",
      " [ 0.90453403  0.34641016]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[5, 20], [1, 50], [3, 80], [5, 60]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "print(scaler.mean_)\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "So Feature scaling is important,ex: If a feature has a variance that is of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='scale_sensitivity2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since X is vector of X1 and X2\n",
    "$$X=(X1,X2)$$\n",
    "so W is Vector os W1 and W2\n",
    "$$W=(W1,W2)$$\n",
    "so if range of X2>>X1  it will dominate the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n",
    "If we strictly impose that all instances be off the street and on the right side, this is\n",
    "called hard margin classification. The issue with hard margin classification that it is quite sensitive\n",
    "to outliers. The figure below shows the iris dataset with just one additional outlier: on\n",
    "the left, it is impossible to find a hard margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='softvshard.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid these issues it is preferable to use a more flexible model. The objective is to\n",
    "find a good balance between keeping the street as large as possible and limiting the\n",
    "margin violations (i.e., instances that end up in the middle of the street or even on the\n",
    "wrong side). This is called soft margin classification.\n",
    "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter:\n",
    "a smaller C value leads to a wider street but more margin violations. The following Figure\n",
    "shows the decision boundaries and margins of two soft margin SVM classifiers on a\n",
    "nonlinearly separable dataset. On the left, using a low C value the margin is quite\n",
    "large, but many instances end up on the street. On the right, using a high C value the\n",
    "classifier makes fewer margin violations but ends up with a smaller margin. However,\n",
    "it seems likely that the first classifier will generalize better: in fact even on this training\n",
    "set it makes fewer prediction errors, since most of the margin violations are\n",
    "actually on the correct side of the decision boundary.\n",
    "<img src='diffC.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your SVM model is overfitting, you can try regularizing it by\n",
    "reducing C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using Support Vector Classifier (SVC) class with C = 1) to detect Iris-Virginica flowers which is the right part in the previous figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", SVC(C=1, kernel=\"linear\")),\n",
    "))\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features in some cases this can result in a linearly separable dataset. Consider the left plot in following Figure : it represents a simple dataset with just one feature x1. This dataset is not linearly separable, as you can see. But if you add a second feature $$x2 = (x1)^2$$, the resulting 2D dataset is perfectly linearly separable.\n",
    "<img src='nonLinearSVM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this idea using Scikit-Learn, you can create a Pipeline containing a PolynomialFeatures transformer, followed by a StandardScaler and a LinearSVC or using SVC with polynomial after standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test this on the moons\n",
    "dataset: this is a toy dataset for binary classification in which the data points are shaped\n",
    "as two interleaving half circles. You can generate this dataset\n",
    "using the make_moons() function:\n",
    "the following are the features of moons dataset\n",
    "<img src='moon_features.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#polynomial_svm_clf =  Pipeline([\n",
    " #       (\"scaler\", StandardScaler()),\n",
    "  #      (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=100, C=5))\n",
    "   # ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the above code but for the dataset of the moon and changing the degree of polynomial from 3 to 100 you will get the following\n",
    "graphs you  can see the Code in Not_required.ipynb file\n",
    "<img src='polynomials.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it. \n",
    "Note: The Right is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of Linear SVM Functions\n",
    "The linear SVM classifier model predicts the class of a new instance x by simply computing\n",
    "the decision function\n",
    "$$w^T.x+b=w1x1+w2x2+...+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the result is positive, the predicted class ŷ is the positive class target variable=1\n",
    "else it is the negative class\n",
    "<img src='equation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure two-dimensional plane since this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (represented by the thick solid line). While, The dashed lines represent the points where the decision function is equal to 1 or –1: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).\n",
    "<img src='decisionfunction.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why to min ||w||\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector,\n",
    "∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\n",
    "to ±1 are going to be twice as far away from the decision boundary. In other words,\n",
    "dividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visualize\n",
    "in 2D in Figure 5-13. The smaller the weight vector w, the larger the margin.\n",
    "<img src='weightsdecreased.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
